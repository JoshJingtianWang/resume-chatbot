{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "After extensive data processing, feature refinement, and model adjustments, we utilized the RandomForest classifi- cation method. Our preprocessing activities for this method involved converting categorical data using one-hot encoding, standardizing numerical data, using TF-IDF for textual con- tent, and analyzing the sentiment of the tweets. By employing a combination of grid search and the Synthetic Minority Over- sampling Technique (SMOTE) to address data imbalance, our RandomForest method attained a score of 0.74772 on Kaggle. We also delved into the use of Transformer-based meth- ods because of their adeptness at managing textual content and their impressive learning capabilities. For our study, we tried several such models, like DistilBERT, DistilBERT Multilingual, XLM-RoBERTa Base, and two intfloat models: multilingual-e5-base and multilingual-e5-small. Each model was adapted to our dataset using their existing parameters. Though all models showed great results, the best accuracy noted was 0.85, clearly outpacing the RandomForest tech- nique.\n\nTherefore, both RandomForest and Transformer-based ap- proaches showed high precision in categorizing tweets. How- ever, the Transformer models proved to be the better choice. The accuracy of the transformer model on the training data is 99.28%, which is higher than the accuracy score on the test data, as expected. The confusion matrix on the training data can be seen below.\n\nFig. 8. The confusion matrix on the training data.", "metadata": {"source": "./documents/University of Rochester/kaggle_northern_europe_politician_tweet_project_report.pdf"}, "type": "Document"}}