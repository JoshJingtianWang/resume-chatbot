{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "and per capita)\n\nPopulation density (calculated using total population / land\n\narea)\n\nPercent uninsured We also noticed that removing the twitter data improved our\n\nmodel performance, so all twitter features were discarded. We applied the following pre-processing techniques: - Leave-One-Out Encoding of the \u201dcounty\u201d column [4]: due to the high cardinality of the \u201dcounty\u201d column, we did not use one-hot encoding. Instead, we used leave-one-out encoding, which replaces a categorical value with the mean of the target variable. Compared to target encoding, leave-one-out encoding excludes the current row\u2019s target when calculating the mean target for a level to reduce the effect of outliers.\n\nYeo-Johnson Power Transformation [5]: this transform data to be more Gaussian-like, which may improve the per- formance of linear regression.\n\nRemove Multi-collinearity: High correlated features were removed. This may reduce dimensionality and improve inter- pretability of our model. Using 10-fold cross validation, Pycaret chose Gradient Boosting Regressor as the top model. For hyperparameter tuning, we used the TPE (Tree-structured Parzen Estimator) algorithm from the Optuna package [6]. This is a type of Bayesian Optimization that has higher speed than the exhaus- tive grid search approach. In the end, this model resulted in an R2 of 0.86592.\n\nFigure 6 shows the feature importance extracted from the\n\nGradient Boosting Regressor model.\n\n2) Ensemble Model Approach: We split the training data\n\nby county and trained a separate model for each county. We applied the following pre-processing techniques: - Normalization: we used Z-score normalization. - Remove Multi-collinearity - Feature Selection: We used sklearn\u2019s SelectKBest [7], which works by selecting the top K features based on their\n\nFig. 6. Feature Importance of the Single Model Approach.\n\nunivariate statistical relationship with the target variable. This might reduce dimensionality and reduce overfitting.", "metadata": {"source": "./documents/University of Rochester/kaggle_ohio_project_report.pdf"}, "type": "Document"}}