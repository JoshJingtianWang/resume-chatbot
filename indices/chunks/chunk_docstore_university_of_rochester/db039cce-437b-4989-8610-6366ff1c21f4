{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "L1 models outperform L2 models\n\nRecursively trained each of the models 25 times. Each model iteration was tested on 20 bootstrapped test datasets.  \n\nUNet L1\n\nUNet L2\n\nGenerator L1\n\nGenerator L2\n\nUV percent error\n\nMSE (normalized)\n\nPage 19\n\n*unet - unet with L1 loss; unet_L2 - unet with L2 loss; generator - pix2pix with l1 loss; generator_L2 - pix2pix with L2 loss\n\npix2pix Generator is better than UNet for low energy outputs\n\nUNet L1\n\nGenerator L1\n\nPage 20\n\nAccuracy looks promising but what about the model\u2019s speed and scalability?\n\nPage 21\n\nIntroducing model trials on\n\nNvidia Ampere A100 40GB GPU !\n\nHuge thanks to Professor Brendon Mort and the compute resources \n\noffered by BlueHive!\n\nPage 22\n\nTo put computing capacity of Nvidia A100 in perspective...\n\nThe NVIDIA A100 has 432 tensor cores and 40 GB of high-bandwidth memory in comparison to RTX 3090 which has 328 tensor cores and 24 GB GDDR6X.\n\nIn layman terms, the A100 can be up to 20 times faster than the previous generations.\n\nIn order to test our model\u2019s platform independence and scalability, we did model trials on the A100 GPU.\n\nhttps://www.aime.info/blog/en/deep-learning-gpu-benchmarks-2020/\n\nPage 23   \n\nHow did A100 perform for our use-case?\n\nTraining time for 1088 image pairs for 300 epochs\n\nMean MSE for UNET : 0.0004\n\nMean MSE for Pix2Pix : 0.0004\n\nAs evident from the training time speedup in the chart beside, the UR-LLE team can utilize high-end GPUs to speed up the algorithm on large datasets without compromising accuracy.\n\nminutes\n\nPage 24  \n\nHow does the model perform on unseen energy pairs?\n\nHold-out test set: Run number 53\n\nTraining dataset: Run numbers 20 - 52  \n\nAccuracy\n\nMean MSE for unet: 0.0004\n\nMean MSE for generator: 0.0008\n\nPage 25 \n\nPaper Publication at the CLEO conference\n\nWe are happy to announce our authorship and  participation in CLEO conference led by Mark. \n\nTechnical Conference:  05 \u2013 10 May 2024\n\nThe CLEO Hub: 07 \u2013 09 May 2024\n\nLocation: Charlotte Convention Center,", "metadata": {"source": "./documents/University of Rochester/GAN_capstone_project_presentation.pptx"}, "type": "Document"}}