{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "the se- lected models was initialized with their respective pre-trained weights. We leveraged the power of transfer learning by fine-tuning these models on our dataset. Fine-tuning involved updating the pre-trained weights based on our specific task of classifying tweets into political leanings.\n\n3) Model Training and Fine-Tuning: Each of\n\nTraining was performed using the AdamW optimizer with a learning rate set empirically to ensure convergence. We split our dataset into training and validation subsets. During training, the models were updated iteratively to minimize the classification loss on the training data. After each epoch, model performance was evaluated on the validation set, and the best- performing model was saved.\n\n4) Evaluation: Post-training, the models were evaluated on a test set that they had never seen during the training phase. The primary metric used for evaluation was accuracy, i.e., the percentage of tweets for which the model\u2019s predicted political leaning matched the actual label. IV. RESULTS", "metadata": {"source": "./documents/University of Rochester/kaggle_northern_europe_politician_tweet_project_report.pdf"}, "type": "Document"}}