{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "2) Model Selection: We selected a range of transformer- based models to evaluate their effectiveness in classifying the political leanings of tweets. The models chosen are known for their multilingual capabilities and generalizability across a wide range of tasks. The models used are:\n\nDistilBERT: A distilled version of BERT (Bidirectional Encoder Representations from Transformers). It\u2019s designed to be lighter and faster while retaining most of BERT\u2019s perfor- mance. It\u2019s especially suited for tasks where computational resources or inference speed are limiting factors.\n\nDistilBERT Multilingual: An extension of DistilBERT, this model is trained on texts from multiple languages, allowing for a more generalized representation of language patterns across diverse linguistic datasets.\n\nXLM-RoBERTa (XLM-R) Base: XLM-RoBERTa is a scaled-up version of the XLM model and RoBERTa training approach. The \u201dbase\u201d variant is a relatively smaller but highly capable version of this model.\n\nis a multilingual intfloat/multilingual-e5-base: This lan- from various trained on texts transformer model guages. The \u201dbase\u201d specification suggests a balance be- tween model size and performance. This model was cho- is one of the top performing multilingual sen because it\n\n\n\nmodels on the HuggingFace Text Classification Leaderboard (https://huggingface.co/spaces/mteb/leaderboard).\n\nintfloat/multilingual-e5-small: A smaller variant of the multilingual model mentioned above, designed for quicker inference times and reduced computational requirements. After extensive testing, we settled on intfloat/multilingual- e5-small, as it achieved a great score on Kaggle while com- pleting the training in reasonable time.", "metadata": {"source": "./documents/University of Rochester/kaggle_northern_europe_politician_tweet_project_report.pdf"}, "type": "Document"}}