{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "Page 7 of 12\n\nJake Brehm \u2022 Josh Wang\n\nUniversity of Rochester\n\nThe top 5 topics and the top 5 keywords of each topic are tabulated in Table 2 as well.\n\nTopic\n\nKeywords (most to least significant)\n\nUnknown1 California Wildfire fire, forest, wild, forest fire, truck Unknown2 Loud Scream Burn Building\n\ngo, get, know, bomb, think\n\nbody, bag, body bag, cross, cross body scream, love, loud, hear, ass burn, building, burn building, fire burn, crash\n\nTable 2: Top 5 keywords for each of the top 5 topics.\n\n4.4 Naives Bayes Classification\n\nNaive Bayes classifiers are a family of multiple individual classification algorithms based on Bayes\u2019 Theorem. These algorithms share a common principle: \u201devery pair of features being classified is independent of each other\u201d.[7]\n\nOut of the collection of naive Bayes classifier algorithms, we decided to use Multi- nomial Naive Bayes (abbreviated MNB, called MultinomialNB in the sklearn Python library). In this implementation, the probabilities P (f eature|class) follow a multinomial distribution (word counts, probabilities, etc.).\n\nFirst, we checked for class imbalance (Figure 6) in our data.\n\nFigure 6: Results of naive Bayes classification.\n\nWe determined that there is no severe class imbalance, so there was no need to use\n\nmethods such as undersampling or oversampling.\n\nTo begin our analysis in Python, we removed stop words from and lemmatized the text using the spacy library. To prevent any data leakage, we incorporated TF-IDF tokenization into an sklearn 5-fold cross-validation pipeline. Hyperparameters were tuned using GridSearchCV, which is part of the sklearn library.\n\nPage 8 of 12\n\nJake Brehm \u2022 Josh Wang\n\nUniversity of Rochester", "metadata": {"source": "./documents/University of Rochester/data mining Final Project (Report).pdf"}, "type": "Document"}}