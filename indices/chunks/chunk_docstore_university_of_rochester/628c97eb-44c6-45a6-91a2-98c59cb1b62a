{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "III. METHODS\n\nA. RandomForest\n\n(The Python code for the RandomForest section was not included in our final submission as we chose the Transformer model as our best model.)\n\n1) Data Prep: - Categorical\n\ngender user, in reply to screen name, url string. We applied one-hot encoding for transformation.\n\nFeatures:\n\ncountry user,\n\nNumeric Features:\n\ntext clean char count, dardized.\n\ntext char count, text words count, text clean words count were stan-\n\nText Feature: text clean was vectorized using the TF-IDF\n\nmethodology.\n\nHashtag Feature: Hashtags were split and transformed\n\nusing the CountVectorizer with a cap of 5,000 features.\n\nURL Feature: URLs in the url string column were ex- tracted from the full text decoded column of the dataset, and were transformed by the CountVectorizer.\n\nthe full text decoded column was extracted using the TextBlob library, focusing on the polarity scores.\n\n\n\nSentiment Analysis:\n\nThe\n\nsentiment\n\nof\n\n2) Model training: We used a grid search to fine-tune the parameters of the RandomForest. Recognizing the potential skew in our data distribution (where the majority of political views are Left), we employed the Synthetic Minority Over- sampling Technique (SMOTE) to address class imbalance in the target variable. The optimization process leveraged 5-fold cross-validation, aiming to optimize the weighted F1 score. By utilizing this module, we achieved a score of 0.74772 in our Kaggle submission. We decided to look for other models to achieve a potentially better accuracy score. B. Transformer\n\n1) Preprocessing: Before feeding the tweets into the trans- former models, they underwent standard preprocessing suit- able for transformer models, which includes decoding the bytes literal into regular string, appending country and gender info to the string, encoding the labels to integers, tokenization, and finally attention mask creation without aggressive clean- ing, allowing the models to make the most of their pre-trained knowledge.", "metadata": {"source": "./documents/University of Rochester/kaggle_northern_europe_politician_tweet_project_report.pdf"}, "type": "Document"}}