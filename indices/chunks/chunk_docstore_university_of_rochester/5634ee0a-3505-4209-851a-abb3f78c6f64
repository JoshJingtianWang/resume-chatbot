{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "Fig. 5. The cleaned data information.\n\nUpon examining the processed text, which was cleaned and lemmatized from the original tweet content, we observe that the distribution remains symmetrical, with the average and median of \u2018clean text\u2019 is about the same.A comparison between the original tweet content and the \u2019clean text\u2019 high- lights certain disparities. Specifically, the minimum length of the \u2019clean text\u2019 stands at zero, encompassing no characters. Conversely, the maximum length extends to 77 words, trans- lating to 752 characters\u2014a marked contrast in character count pre-cleaning. It is postulated that a significant portion of this difference stems from link characters, given that we probably eliminated a lot links during the cleaning phase.\n\nBy comparing the results shown in Figure 6 and Figure 7, we can see that both Latent Dirichlet Allocation (LDA)\n\nFig. 6. Latent Dirichlet allocation (LDA)\n\nFig. 7. Non-negative matrix factorization (NMF)\n\nand Non-negative Matrix Factorization (NMF) seem to identify similar topics in the data. The most significant words in the topics yield similar results, with words such as \u2019dkpol\u2019, \u2019amp\u2019, \u2019via\u2019, and \u2019bra\u2019 appearing in both modules. We can inspect the top words for each topic and observe that those in NMF are more distinct, which can help us gain a clearer idea of its theme. If clarity of topics is a priority, NMF seems to have an edge in this dataset. From a computational standpoint, NMF is considerably faster, taking only 24 seconds, which is roughly 18% of the time LDA requires, completing in 128 seconds. This difference in execution time might have implications for scalability, especially when processing larger datasets. Therefore, Non-negative Matrix Factorization (NMF) would be considered the better option for our dataset.\n\nIII. METHODS\n\nA. RandomForest\n\n(The Python code for the RandomForest section was not included in our final submission as we chose the Transformer model as our best model.)", "metadata": {"source": "./documents/University of Rochester/kaggle_northern_europe_politician_tweet_project_report.pdf"}, "type": "Document"}}