{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "Page 9 of 12\n\nJake Brehm \u2022 Josh Wang\n\nUniversity of Rochester\n\n3-fold cross validation and trained the LSTM in 12 epochs for each fold. Both accuracy and F1-score were considered.\n\nOur results are shown in Figure 7.\n\n(a) Loss of validation set.\n\n(b) Accuracy of validation set.\n\n(c) F1-score of validation set.\n\nFigure 7: Results of LSTM model analysis.\n\nAs illustrated by the three plots, the loss, accuracy, and F1 in the validation set level off at around epoch 8. Note that missing data in the train F1 line in the F1 plot is due to division by zero in the calculation of F1, which was caused by misclassifications early in the epochs.\n\nMean Validation Loss: 0.5765 Mean Validation Accuracy: 0.7638 Mean Validation F1: 0.6692 EMBEDDING_DIM: 300 NUM_HIDDEN_NODES: 64 NUM_LAYERS: 3 BIDIRECTION: True DROPOUT: 0.3 NUM_HIDDEN_NODES: 64\n\nRemoving URLs, mentions, HTML entities, and most non-alphanumeric characters from the tweets did not improve the performance of the model.\n\nOverall, naive Bayes classification performed better than LSTM. This is possibly due to the small size of the dataset; deep learning models usually benefit from a large dataset, while machine learning techniques such as naive Bayes classification are more suitable for small data sets such as this one. In addition, naive Bayes models have fewer hyperpa- rameters compared to LSTM, and our naive Bayes model has the benefit of Grid Search tuning.\n\n5 Future Work\n\nIn the future, we could try using other classification models such as random forest (after vectorization of the text data), deep learning models such as GRU, and transformers such as BERT.\n\nPage 10 of 12\n\nJake Brehm \u2022 Josh Wang\n\nUniversity of Rochester", "metadata": {"source": "./documents/University of Rochester/data mining Final Project (Report).pdf"}, "type": "Document"}}