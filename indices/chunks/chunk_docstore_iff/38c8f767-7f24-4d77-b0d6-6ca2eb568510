{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "Small dataset size: IFF Vendor Data has ~500,000 entries, only 1,000 of which have been labeled\n\nModel trained on a labeled small subset of data has sub-par performance.\n\nsource: https://towardsdatascience.com/introduction-to-active-learning-117e0740d7cc\n\nAvoiding False Positive\n\nInstead of making decisions based a one-off result, we evaluate two models on multiple subsets of test data.\n\nTwo lists of accuracy scores\n\nMean\n\nVariance\n\nSample size\n\nGreater difference in means,\n\nSmaller the variance,\n\nGreater the sample sizes\n\nMore likely it is to detect the difference between two populations\n\nLow false positive rate \uf0e0 high confidence\n\nAlpha = 0.05 (you're willing to accept a 5% chance of obtaining a false positive result)\n\nAvoiding False Negative\n\nIf a real difference in performance exists between two models, we want to detect that difference as often as possible.\n\nProbability to detect the real difference \uf0e0 1 - false negative rate \uf0e0 Power\n\ne.g. \n\na t test with a power of 0.80 \uf0e0 80% of the time it can correctly detect the difference in the two models. 20% chance of missing that difference (false negative)\n\nPowerSimulation module\n\nHolding desired power constant, we can estimate the minimum test data size required to achieve that power.\n\nModelCompare module\n\nAccepts:\n\nOld model\n\nNew model\n\nTest data\n\nEvaluation metric\n\nAlpha value\n\nReturns:\n\nT statistic\n\np value\n\nModelCompare Code demo\n\nImport \n\nModules\n\nGet data\n\nGet models\n\nRunning\n\nModelCompare", "metadata": {"source": "./documents/IFF/final presentation.pptx"}, "type": "Document"}}