{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "from typing import List, Tuple, Union import numpy as np import pandas as pd import scipy.stats as stats from sklearn.base import ClassifierMixin, RegressorMixin from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score from tqdm import tqdm import logging\n\nclass ModelCompare: \"\"\" Compares the performance of two models using a specified evaluation metric, and performs a statistical hypothesis test to determine if there is a significant difference between the models' performances.\n\nParameters ---------- evaluation_metric : str, default='accuracy' The evaluation metric to use for comparing the models. Must be one of 'accuracy', 'f1', 'recall', 'roc', 'mae', 'mse', 'r2'. alpha : float, default=0.05 The significance level for the hypothesis test. Must be between 0 and 1. one_sided : bool, default=True Whether to perform a one-sided or two-sided hypothesis test. seed : int, optional, default=None A random seed for reproducibility. Must be an integer or None. bootstrap_num_iter : int, default=1000 The number of iterations for the bootstrap test. Must be a positive integer. bootstrap_percent : float, default=0.5 The proportion of samples to use in each bootstrap iteration. Must be between 0 and 1 (exclusive).", "metadata": {"source": "./documents/IFF/model_compare.py"}, "type": "Document"}}